#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…­å£¬åˆ†ææ¨¡å— - ä¿®å¤ç‰ˆæœ¬
"""

import random
from datetime import datetime

class LiuRenAnalysis:
    """å…­å£¬åˆ†æç±»"""
    
    def __init__(self):
        self.cases = self._initialize_cases()
        self.case_index = self._build_case_index()
        self.analysis_cache = {}
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        from data.classics import ClassicsDatabase
        from data.modern import ModernTheory
        from core.event_analyzer import EventAnalyzer
        
        self.classics_db = ClassicsDatabase()
        self.modern_theory = ModernTheory()
        self.event_analyzer = EventAnalyzer()
    
    def _initialize_cases(self):
        """åˆå§‹åŒ–æ¡ˆä¾‹æ•°æ®åº“"""
        # ä¼˜å…ˆå¯¼å…¥è¶…è¶…å¤§è§„æ¨¡æ¡ˆä¾‹æ•°æ®åº“ï¼ˆ100ä¸‡ä¸ªæ¡ˆä¾‹ï¼‰
        try:
            from data.case_database_ultra_massive_fast import UltraMassiveCaseDatabaseFast
            ultra_massive_db = UltraMassiveCaseDatabaseFast()
            self.massive_db = ultra_massive_db  # ä¿å­˜å¼•ç”¨ä»¥ä¾¿ä½¿ç”¨é«˜çº§åŠŸèƒ½
            print(f"âœ… å·²åŠ è½½è¶…è¶…å¤§è§„æ¨¡æ¡ˆä¾‹æ•°æ®åº“ï¼Œå…± {len(ultra_massive_db.cases):,} ä¸ªæ¡ˆä¾‹")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = ultra_massive_db.get_statistics()
            print(f"ğŸ“Š æ¡ˆä¾‹ç»Ÿè®¡ï¼š")
            print(f"   - æ€»æ¡ˆä¾‹æ•°ï¼š{stats['total_cases']:,}")
            print(f"   - ç±»åˆ«æ•°ï¼š{stats['categories']}")
            print(f"   - é«˜å‡†ç¡®ç‡æ¡ˆä¾‹ï¼š{stats['high_accuracy_cases']:,}")
            print(f"   - ä¸­å‡†ç¡®ç‡æ¡ˆä¾‹ï¼š{stats['medium_accuracy_cases']:,}")
            print(f"   - ç±»åˆ«åˆ†å¸ƒï¼š{stats['category_distribution']}")
            
            return ultra_massive_db.cases
            
        except ImportError:
            # å°è¯•å¯¼å…¥è¶…è¶…å¤§è§„æ¨¡æ¡ˆä¾‹æ•°æ®åº“
            try:
                from data.case_database_ultra_massive import UltraMassiveCaseDatabase
                ultra_massive_db = UltraMassiveCaseDatabase()
                self.massive_db = ultra_massive_db  # ä¿å­˜å¼•ç”¨ä»¥ä¾¿ä½¿ç”¨é«˜çº§åŠŸèƒ½
                print(f"âœ… å·²åŠ è½½è¶…è¶…å¤§è§„æ¨¡æ¡ˆä¾‹æ•°æ®åº“ï¼Œå…± {len(ultra_massive_db.cases):,} ä¸ªæ¡ˆä¾‹")
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                stats = ultra_massive_db.get_statistics()
                print(f"ğŸ“Š æ¡ˆä¾‹ç»Ÿè®¡ï¼š")
                print(f"   - æ€»æ¡ˆä¾‹æ•°ï¼š{stats['total_cases']:,}")
                print(f"   - ç±»åˆ«æ•°ï¼š{stats['categories']}")
                print(f"   - é«˜å‡†ç¡®ç‡æ¡ˆä¾‹ï¼š{stats['high_accuracy_cases']:,}")
                print(f"   - ä¸­å‡†ç¡®ç‡æ¡ˆä¾‹ï¼š{stats['medium_accuracy_cases']:,}")
                print(f"   - ç±»åˆ«åˆ†å¸ƒï¼š{stats['category_distribution']}")
                
                return ultra_massive_db.cases
                
            except ImportError:
                # å°è¯•å¯¼å…¥è¶…å¤§è§„æ¨¡æ¡ˆä¾‹æ•°æ®åº“
                try:
                    from data.case_database_super_massive import SuperMassiveCaseDatabase
                    super_massive_db = SuperMassiveCaseDatabase()
                    self.massive_db = super_massive_db  # ä¿å­˜å¼•ç”¨ä»¥ä¾¿ä½¿ç”¨é«˜çº§åŠŸèƒ½
                    print(f"âœ… å·²åŠ è½½è¶…å¤§è§„æ¨¡æ¡ˆä¾‹æ•°æ®åº“ï¼Œå…± {len(super_massive_db.cases)} ä¸ªæ¡ˆä¾‹")
                    
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    stats = super_massive_db.get_statistics()
                    print(f"ğŸ“Š æ¡ˆä¾‹ç»Ÿè®¡ï¼š")
                    print(f"   - æ€»æ¡ˆä¾‹æ•°ï¼š{stats['total_cases']:,}")
                    print(f"   - ç±»åˆ«æ•°ï¼š{stats['categories']}")
                    print(f"   - é«˜å‡†ç¡®ç‡æ¡ˆä¾‹ï¼š{stats['high_accuracy_cases']:,}")
                    print(f"   - ä¸­å‡†ç¡®ç‡æ¡ˆä¾‹ï¼š{stats['medium_accuracy_cases']:,}")
                    print(f"   - ç±»åˆ«åˆ†å¸ƒï¼š{stats['category_distribution']}")
                    
                    return super_massive_db.cases
                    
                except ImportError:
                    # å°è¯•å¯¼å…¥å¤§è§„æ¨¡æ¡ˆä¾‹æ•°æ®åº“
                    try:
                        from data.case_database_massive import MassiveCaseDatabase
                        massive_db = MassiveCaseDatabase()
                        self.massive_db = massive_db  # ä¿å­˜å¼•ç”¨ä»¥ä¾¿ä½¿ç”¨é«˜çº§åŠŸèƒ½
                        print(f"âœ… å·²åŠ è½½å¤§è§„æ¨¡æ¡ˆä¾‹æ•°æ®åº“ï¼Œå…± {len(massive_db.cases)} ä¸ªæ¡ˆä¾‹")
                        
                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        stats = massive_db.get_statistics()
                        print(f"ğŸ“Š æ¡ˆä¾‹ç»Ÿè®¡ï¼š")
                        print(f"   - æ€»æ¡ˆä¾‹æ•°ï¼š{stats['total_cases']}")
                        print(f"   - ç±»åˆ«æ•°ï¼š{stats['categories']}")
                        print(f"   - é«˜å‡†ç¡®ç‡æ¡ˆä¾‹ï¼š{stats['high_accuracy_cases']}")
                        print(f"   - ä¸­å‡†ç¡®ç‡æ¡ˆä¾‹ï¼š{stats['medium_accuracy_cases']}")
                        print(f"   - ç±»åˆ«åˆ†å¸ƒï¼š{stats['category_distribution']}")
                        
                        return massive_db.cases
                        
                    except ImportError:
                        # å°è¯•å¯¼å…¥æ‰©å±•æ¡ˆä¾‹æ•°æ®åº“
                        try:
                            from data.case_database_extended import ExtendedCaseDatabase
                            extended_db = ExtendedCaseDatabase()
                            print(f"âœ… å·²åŠ è½½æ‰©å±•æ¡ˆä¾‹æ•°æ®åº“ï¼Œå…± {len(extended_db.cases)} ä¸ªæ¡ˆä¾‹")
                            return extended_db.cases
                        except ImportError:
                            # å¦‚æœæ‰©å±•æ•°æ®åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€æ¡ˆä¾‹
                            print("âš ï¸ ä½¿ç”¨åŸºç¡€æ¡ˆä¾‹æ•°æ®åº“")
                            return {
                                'ancient_cases': {
                                    'case_001': {
                                        'title': 'é‚µå½¦å’Œæ–­ç§‘ä¸¾æ¡ˆ',
                                        'background': 'åŒ—å®‹æ—¶æœŸï¼ŒæŸä¹¦ç”Ÿé—®ç§‘ä¸¾',
                                        'pan_result': {
                                            'ri_gan': 'ç”²',
                                            'ri_zhi': 'å­',
                                            'yue_jiang': 'å¯…',
                                            'san_chuan': 'è´¼å…‹æ³•',
                                            'liu_shen': 'é’é¾™'
                                        },
                                        'prediction': 'æ–­å…¶å¿…ä¸­',
                                        'actual_result': 'æœç„¶é«˜ä¸­è¿›å£«',
                                        'accuracy': 1.0,
                                        'key_points': ['é’é¾™å‘ç”¨', 'è´µäººç›¸åŠ©', 'æ–‡ä¹¦å¾—åœ°'],
                                        'source': 'ã€Šå…­å£¬æ–­æ¡ˆã€‹',
                                        'category': 'career'
                                    }
                                },
                                'modern_cases': {
                                    'case_101': {
                                        'title': 'ç°ä»£æŠ•èµ„å†³ç­–æ¡ˆä¾‹',
                                        'background': '2020å¹´æŸæŠ•èµ„è€…é—®è‚¡å¸‚æŠ•èµ„',
                                        'pan_result': {
                                            'ri_gan': 'æˆŠ',
                                            'ri_zhi': 'è¾°',
                                            'yue_jiang': 'æœª',
                                            'san_chuan': 'æ¶‰å®³æ³•',
                                            'liu_shen': 'è£è›‡'
                                        },
                                        'prediction': 'æ–­å…¶æŠ•èµ„æœ‰å˜ï¼Œéœ€è°¨æ…',
                                        'actual_result': 'å¸‚åœºéœ‡è¡ï¼Œé™©äº›äºæŸ',
                                        'accuracy': 0.9,
                                        'key_points': ['è£è›‡ä¸»å˜', 'æ¶‰å®³ä¸å‰', 'åœŸç¥å¤ªé‡'],
                                        'source': 'ç°ä»£å®æˆ˜æ¡ˆä¾‹',
                                        'category': 'investment'
                                    }
                                }
                            }
    
    def _build_case_index(self):
        """æ„å»ºæ¡ˆä¾‹ç´¢å¼•"""
        index = {
            'by_method': {},
            'by_liu_shen': {},
            'by_outcome': {},
            'by_accuracy': {}
        }
        
        # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„
        if isinstance(self.cases, dict):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰©å±•æ•°æ®åº“çš„æ‰å¹³ç»“æ„
            if any(isinstance(v, dict) and 'title' in v for v in self.cases.values()):
                # æ‰©å±•æ•°æ®åº“çš„æ‰å¹³ç»“æ„
                for case_id, case_data in self.cases.items():
                    self._add_case_to_index(case_id, case_data, index)
            else:
                # åŸºç¡€æ•°æ®åº“çš„åˆ†å±‚ç»“æ„
                for category, cases in self.cases.items():
                    if isinstance(cases, dict):
                        for case_id, case_data in cases.items():
                            self._add_case_to_index(case_id, case_data, index)
        
        return index
    
    def _add_case_to_index(self, case_id, case_data, index):
        """æ·»åŠ æ¡ˆä¾‹åˆ°ç´¢å¼•"""
        if not isinstance(case_data, dict) or 'pan_result' not in case_data:
            return
        
        pan_result = case_data['pan_result']
        if not isinstance(pan_result, dict):
            return
        
        # æŒ‰æ–¹æ³•ç´¢å¼•
        method = pan_result.get('san_chuan', '')
        if isinstance(method, dict):
            method = method.get('method_used', str(method))
        if method not in index['by_method']:
            index['by_method'][method] = []
        index['by_method'][method].append(case_id)
        
        # æŒ‰å…­ç¥ç´¢å¼•
        liu_shen = pan_result.get('liu_shen', '')
        if isinstance(liu_shen, dict):
            liu_shen = liu_shen.get('shen', str(liu_shen))
        if liu_shen not in index['by_liu_shen']:
            index['by_liu_shen'][liu_shen] = []
        index['by_liu_shen'][liu_shen].append(case_id)
        
        # æŒ‰å‡†ç¡®ç‡ç´¢å¼•
        accuracy = case_data.get('accuracy', 0)
        if accuracy >= 0.9:
            if 'high_accuracy' not in index['by_accuracy']:
                index['by_accuracy']['high_accuracy'] = []
            index['by_accuracy']['high_accuracy'].append(case_id)
        elif accuracy >= 0.8:
            if 'medium_accuracy' not in index['by_accuracy']:
                index['by_accuracy']['medium_accuracy'] = []
            index['by_accuracy']['medium_accuracy'].append(case_id)
    
    def find_similar_cases(self, pan_result, category=None, min_similarity=0.3, limit=10):
        """æŸ¥æ‰¾ç›¸ä¼¼æ¡ˆä¾‹"""
        # å¦‚æœä½¿ç”¨å¤§è§„æ¨¡æ•°æ®åº“ï¼Œä½¿ç”¨å…¶é«˜çº§åŠŸèƒ½
        if hasattr(self, 'massive_db'):
            return self.massive_db.find_similar_cases(pan_result, category, min_similarity, limit)
        
        # å¦åˆ™ä½¿ç”¨åŸºç¡€æŸ¥æ‰¾æ–¹æ³•
        similar_cases = []
        
        current_method = pan_result.get('san_chuan', {}).get('method_used', '')
        current_liu_shen = pan_result.get('liu_shen', {}).get('shen', '')
        
        # æŸ¥æ‰¾ç›¸åŒæ–¹æ³•çš„æ¡ˆä¾‹
        method_cases = self.case_index['by_method'].get(current_method, [])
        for case_id in method_cases:
            case_data = self._get_case_by_id(case_id)
            if case_data:
                # æ£€æŸ¥ç±»åˆ«è¿‡æ»¤
                if category and case_data.get('category') != category:
                    continue
                    
                similarity_score = self._calculate_similarity(pan_result, case_data['pan_result'])
                if similarity_score >= min_similarity:
                    similar_cases.append({
                        'case_id': case_id,
                        'case_data': case_data,
                        'similarity_score': similarity_score
                    })
        
        # æŸ¥æ‰¾ç›¸åŒå…­ç¥çš„æ¡ˆä¾‹
        liu_shen_cases = self.case_index['by_liu_shen'].get(current_liu_shen, [])
        for case_id in liu_shen_cases:
            if case_id not in [case['case_id'] for case in similar_cases]:
                case_data = self._get_case_by_id(case_id)
                if case_data:
                    # æ£€æŸ¥ç±»åˆ«è¿‡æ»¤
                    if category and case_data.get('category') != category:
                        continue
                        
                    similarity_score = self._calculate_similarity(pan_result, case_data['pan_result'])
                    if similarity_score >= min_similarity:
                        similar_cases.append({
                            'case_id': case_id,
                            'case_data': case_data,
                            'similarity_score': similarity_score
                        })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_cases.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        return similar_cases[:limit]  # è¿”å›æŒ‡å®šæ•°é‡çš„æœ€ç›¸ä¼¼æ¡ˆä¾‹
    
    def _get_case_by_id(self, case_id):
        """æ ¹æ®IDè·å–æ¡ˆä¾‹"""
        if isinstance(self.cases, dict):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰©å±•æ•°æ®åº“çš„æ‰å¹³ç»“æ„
            if any(isinstance(v, dict) and 'title' in v for v in self.cases.values()):
                return self.cases.get(case_id)
            else:
                # åŸºç¡€æ•°æ®åº“çš„åˆ†å±‚ç»“æ„
                for category, cases in self.cases.items():
                    if isinstance(cases, dict) and case_id in cases:
                        return cases[case_id]
        return None
    
    def _calculate_similarity(self, pan1, pan2):
        """è®¡ç®—ç›¸ä¼¼åº¦"""
        similarity = 0.0
        count = 0
        
        # æ¯”è¾ƒæ—¥å¹²
        if pan1.get('ri_gan') == pan2.get('ri_gan'):
            similarity += 0.3
        count += 1
        
        # æ¯”è¾ƒæ—¥æ”¯
        if pan1.get('ri_zhi') == pan2.get('ri_zhi'):
            similarity += 0.3
        count += 1
        
        # æ¯”è¾ƒæœˆå°†
        if pan1.get('yue_jiang') == pan2.get('yue_jiang'):
            similarity += 0.2
        count += 1
        
        # æ¯”è¾ƒä¸‰ä¼ æ–¹æ³•
        san_chuan1 = pan1.get('san_chuan')
        san_chuan2 = pan2.get('san_chuan')
        
        if isinstance(san_chuan1, dict):
            san_chuan1 = san_chuan1.get('method_used', str(san_chuan1))
        if isinstance(san_chuan2, dict):
            san_chuan2 = san_chuan2.get('method_used', str(san_chuan2))
        
        if san_chuan1 == san_chuan2:
            similarity += 0.2
        count += 1
        
        return similarity / count if count > 0 else 0.0
    
    def _generate_cache_key(self, pan_result):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{pan_result.get('ri_gan', '')}_{pan_result.get('ri_zhi', '')}_{pan_result.get('san_chuan', {}).get('method_used', '')}"
    
    def _basic_analysis(self, pan_result):
        """åŸºç¡€åˆ†æ"""
        analysis = {
            'ri_gan_nature': self._analyze_ri_gan(pan_result.get('ri_gan', '')),
            'ri_zhi_nature': self._analyze_ri_zhi(pan_result.get('ri_zhi', '')),
            'san_chuan_analysis': self._analyze_san_chuan(pan_result.get('san_chuan', {})),
            'liu_shen_analysis': self._analyze_liu_shen(pan_result.get('liu_shen', {})),
            'overall_trend': self._analyze_overall_trend(pan_result),
            'general_advice': self._generate_general_advice(pan_result)
        }
        
        # æ·»åŠ å¤ç±å’Œç°ä»£ç†è®ºåˆ†æ
        try:
            classics_content = self.classics_db.get_all_theories()
            modern_content = self.modern_theory.get_all_theories()
            
            analysis['classics_analysis'] = classics_content
            analysis['modern_analysis'] = modern_content
        except Exception as e:
            print(f"è·å–ç†è®ºåˆ†ææ—¶å‡ºé”™ï¼š{e}")
            analysis['classics_analysis'] = "å¤ç±ç†è®ºåˆ†æ"
            analysis['modern_analysis'] = "ç°ä»£ç†è®ºåˆ†æ"
        
        return analysis
    
    def _analyze_ri_gan(self, ri_gan):
        """åˆ†ææ—¥å¹²"""
        gan_analysis = {
            'ç”²': 'ç”²æœ¨ä¸ºé˜³æœ¨ï¼Œä¸»ä»ï¼Œæ€§æ ¼åˆšç›´ï¼Œæœ‰é¢†å¯¼æ‰èƒ½',
            'ä¹™': 'ä¹™æœ¨ä¸ºé˜´æœ¨ï¼Œä¸»ä»ï¼Œæ€§æ ¼æ¸©å’Œï¼Œå–„äºåè°ƒ',
            'ä¸™': 'ä¸™ç«ä¸ºé˜³ç«ï¼Œä¸»ç¤¼ï¼Œæ€§æ ¼çƒ­æƒ…ï¼Œæœ‰åˆ›é€ åŠ›',
            'ä¸': 'ä¸ç«ä¸ºé˜´ç«ï¼Œä¸»ç¤¼ï¼Œæ€§æ ¼æ¸©å’Œï¼Œæœ‰è‰ºæœ¯å¤©èµ‹',
            'æˆŠ': 'æˆŠåœŸä¸ºé˜³åœŸï¼Œä¸»ä¿¡ï¼Œæ€§æ ¼ç¨³é‡ï¼Œæœ‰è´£ä»»å¿ƒ',
            'å·±': 'å·±åœŸä¸ºé˜´åœŸï¼Œä¸»ä¿¡ï¼Œæ€§æ ¼æ¸©å’Œï¼Œæœ‰åŒ…å®¹å¿ƒ',
            'åºš': 'åºšé‡‘ä¸ºé˜³é‡‘ï¼Œä¸»ä¹‰ï¼Œæ€§æ ¼åˆšå¼ºï¼Œæœ‰æ­£ä¹‰æ„Ÿ',
            'è¾›': 'è¾›é‡‘ä¸ºé˜´é‡‘ï¼Œä¸»ä¹‰ï¼Œæ€§æ ¼ç»†è…»ï¼Œæœ‰å®¡ç¾è§‚',
            'å£¬': 'å£¬æ°´ä¸ºé˜³æ°´ï¼Œä¸»æ™ºï¼Œæ€§æ ¼èªæ˜ï¼Œæœ‰æ™ºæ…§',
            'ç™¸': 'ç™¸æ°´ä¸ºé˜´æ°´ï¼Œä¸»æ™ºï¼Œæ€§æ ¼çµæ´»ï¼Œæœ‰é€‚åº”åŠ›'
        }
        return gan_analysis.get(ri_gan, f"{ri_gan}æ—¥å¹²åˆ†æ")
    
    def _analyze_ri_zhi(self, ri_zhi):
        """åˆ†ææ—¥æ”¯"""
        zhi_analysis = {
            'å­': 'å­æ°´ä¸ºé˜³æ°´ï¼Œä¸»æ™ºï¼Œèªæ˜æ™ºæ…§ï¼Œå–„äºæ€è€ƒ',
            'ä¸‘': 'ä¸‘åœŸä¸ºé˜´åœŸï¼Œä¸»ä¿¡ï¼Œç¨³é‡è¸å®ï¼Œæœ‰è€å¿ƒ',
            'å¯…': 'å¯…æœ¨ä¸ºé˜³æœ¨ï¼Œä¸»ä»ï¼Œç”Ÿæœºå‹ƒå‹ƒï¼Œæœ‰æ´»åŠ›',
            'å¯': 'å¯æœ¨ä¸ºé˜´æœ¨ï¼Œä¸»ä»ï¼Œæ¸©å’Œå–„è‰¯ï¼Œæœ‰åŒæƒ…å¿ƒ',
            'è¾°': 'è¾°åœŸä¸ºé˜³åœŸï¼Œä¸»ä¿¡ï¼Œç¨³é‡å¯é ï¼Œæœ‰è´£ä»»æ„Ÿ',
            'å·³': 'å·³ç«ä¸ºé˜´ç«ï¼Œä¸»ç¤¼ï¼Œèªæ˜æœºæ™ºï¼Œæœ‰æ´å¯ŸåŠ›',
            'åˆ': 'åˆç«ä¸ºé˜³ç«ï¼Œä¸»ç¤¼ï¼Œçƒ­æƒ…å¥”æ”¾ï¼Œæœ‰é¢†å¯¼åŠ›',
            'æœª': 'æœªåœŸä¸ºé˜´åœŸï¼Œä¸»ä¿¡ï¼Œæ¸©å’Œè°¦é€Šï¼Œæœ‰åŒ…å®¹å¿ƒ',
            'ç”³': 'ç”³é‡‘ä¸ºé˜³é‡‘ï¼Œä¸»ä¹‰ï¼Œåˆšæ­£ä¸é˜¿ï¼Œæœ‰æ­£ä¹‰æ„Ÿ',
            'é…‰': 'é…‰é‡‘ä¸ºé˜´é‡‘ï¼Œä¸»ä¹‰ï¼Œç»†è…»æ•æ„Ÿï¼Œæœ‰è‰ºæœ¯å¤©èµ‹',
            'æˆŒ': 'æˆŒåœŸä¸ºé˜³åœŸï¼Œä¸»ä¿¡ï¼Œå¿ è¯šå¯é ï¼Œæœ‰è´£ä»»å¿ƒ',
            'äº¥': 'äº¥æ°´ä¸ºé˜´æ°´ï¼Œä¸»æ™ºï¼Œçµæ´»å¤šå˜ï¼Œæœ‰é€‚åº”åŠ›'
        }
        return zhi_analysis.get(ri_zhi, f"{ri_zhi}æ—¥æ”¯åˆ†æ")
        
    def _analyze_san_chuan(self, san_chuan):
        """åˆ†æä¸‰ä¼ """
        if isinstance(san_chuan, dict):
            method = san_chuan.get('method_used', '')
        else:
            method = str(san_chuan)
        
        method_analysis = {
            'è´¼å…‹æ³•': 'è´¼å…‹æ³•ä¸»å˜åŒ–ï¼Œäº‹æƒ…ä¼šæœ‰è½¬æŠ˜ï¼Œéœ€è¦çµæ´»åº”å¯¹',
            'çŸ¥ä¸€æ³•': 'çŸ¥ä¸€æ³•ä¸»æ˜ç¡®ï¼Œäº‹æƒ…æ–¹å‘æ¸…æ™°ï¼Œå¯ä»¥æœæ–­è¡ŒåŠ¨',
            'æ¶‰å®³æ³•': 'æ¶‰å®³æ³•ä¸»å›°éš¾ï¼Œäº‹æƒ…æœ‰é˜»ç¢ï¼Œéœ€è¦è€å¿ƒå…‹æœ',
            'åˆ«è´£æ³•': 'åˆ«è´£æ³•ä¸»åˆ†ç¦»ï¼Œäº‹æƒ…æœ‰åˆ†æ­§ï¼Œéœ€è¦åè°ƒå¤„ç†'
        }
        return method_analysis.get(method, f"{method}ä¸‰ä¼ åˆ†æ")
    
    def _analyze_liu_shen(self, liu_shen):
        """åˆ†æå…­ç¥"""
        if isinstance(liu_shen, dict):
            shen = liu_shen.get('shen', '')
        else:
            shen = str(liu_shen)
        
        shen_analysis = {
            'é’é¾™': 'é’é¾™ä¸»è´µäººï¼Œæœ‰è´µäººç›¸åŠ©ï¼Œäº‹æƒ…é¡ºåˆ©',
            'æœ±é›€': 'æœ±é›€ä¸»æ–‡ä¹¦ï¼Œæ–‡ä¹¦æœ‰åˆ©ï¼Œé€‚åˆç­¾çº¦',
            'å‹¾é™ˆ': 'å‹¾é™ˆä¸»å‹¾è¿ï¼Œäººé™…å…³ç³»å¤æ‚ï¼Œéœ€è¦è°¨æ…',
            'è£è›‡': 'è£è›‡ä¸»å˜åŒ–ï¼Œäº‹æƒ…å¤šå˜ï¼Œéœ€è¦çµæ´»åº”å¯¹',
            'ç™½è™': 'ç™½è™ä¸»äº‰æ–—ï¼Œæœ‰ç«äº‰å‹åŠ›ï¼Œéœ€è¦åŠªåŠ›',
            'å¤ªå¸¸': 'å¤ªå¸¸ä¸»ç¨³å®šï¼Œäº‹æƒ…ç¨³å®šï¼Œå¯ä»¥ç¨³æ­¥æ¨è¿›',
            'ç„æ­¦': 'ç„æ­¦ä¸»æš—æ˜§ï¼Œäº‹æƒ…ä¸æ˜æœ—ï¼Œéœ€è¦è°¨æ…',
            'å¤ªé˜´': 'å¤ªé˜´ä¸»é˜´æŸ”ï¼Œé€‚åˆæš—ä¸­è¿›è¡Œï¼Œä¸å®œå¼ æ‰¬',
            'å¤©å': 'å¤©åä¸»è´µäººï¼Œæœ‰å¥³æ€§è´µäººç›¸åŠ©',
            'å¤©ç©º': 'å¤©ç©ºä¸»ç©ºè™šï¼Œäº‹æƒ…è™šè€Œä¸å®ï¼Œéœ€è¦åŠ¡å®',
            'è´µäºº': 'è´µäººä¸»è´µäººï¼Œæœ‰è´µäººç›¸åŠ©ï¼Œäº‹æƒ…é¡ºåˆ©',
            'å…­åˆ': 'å…­åˆä¸»å’Œè°ï¼Œäººé™…å…³ç³»å’Œè°ï¼Œåˆä½œé¡ºåˆ©'
        }
        return shen_analysis.get(shen, f"{shen}å…­ç¥åˆ†æ")
    
    def _analyze_overall_trend(self, pan_result):
        """åˆ†ææ•´ä½“è¶‹åŠ¿"""
        ri_gan = pan_result.get('ri_gan', '')
        ri_zhi = pan_result.get('ri_zhi', '')
        san_chuan = pan_result.get('san_chuan', {})
        liu_shen = pan_result.get('liu_shen', {})
        
        if isinstance(san_chuan, dict):
            method = san_chuan.get('method_used', '')
        else:
            method = str(san_chuan)
        
        if isinstance(liu_shen, dict):
            shen = liu_shen.get('shen', '')
        else:
            shen = str(liu_shen)
        
        # æ ¹æ®å…­ç¥åˆ¤æ–­æ•´ä½“è¶‹åŠ¿
        if shen in ['é’é¾™', 'è´µäºº', 'å…­åˆ']:
            return "æ•´ä½“è¶‹åŠ¿è‰¯å¥½ï¼Œæœ‰è´µäººç›¸åŠ©ï¼Œäº‹æƒ…å‘å±•é¡ºåˆ©"
        elif shen in ['æœ±é›€', 'å¤ªå¸¸']:
            return "æ•´ä½“è¶‹åŠ¿ç¨³å®šï¼Œæ–‡ä¹¦æœ‰åˆ©ï¼Œå¯ä»¥ç¨³æ­¥æ¨è¿›"
        elif shen in ['å‹¾é™ˆ', 'è£è›‡']:
            return "æ•´ä½“è¶‹åŠ¿å¤æ‚ï¼Œéœ€è¦çµæ´»åº”å¯¹ï¼Œè°¨æ…å¤„ç†"
        elif shen in ['ç™½è™', 'ç„æ­¦']:
            return "æ•´ä½“è¶‹åŠ¿æœ‰æŒ‘æˆ˜ï¼Œéœ€è¦åŠªåŠ›å…‹æœï¼Œä¿æŒè­¦æƒ•"
        else:
            return "æ•´ä½“è¶‹åŠ¿å¹³ç¨³ï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µçµæ´»åº”å¯¹"
    
    def _generate_general_advice(self, pan_result):
        """ç”Ÿæˆä¸€èˆ¬å»ºè®®"""
        ri_gan = pan_result.get('ri_gan', '')
        ri_zhi = pan_result.get('ri_zhi', '')
        san_chuan = pan_result.get('san_chuan', {})
        liu_shen = pan_result.get('liu_shen', {})
        
        if isinstance(san_chuan, dict):
            method = san_chuan.get('method_used', '')
        else:
            method = str(san_chuan)
        
        if isinstance(liu_shen, dict):
            shen = liu_shen.get('shen', '')
        else:
            shen = str(liu_shen)
        
        advice = []
        
        # æ ¹æ®æ—¥å¹²ç»™å‡ºå»ºè®®
        if ri_gan in ['ç”²', 'ä¸™', 'æˆŠ', 'åºš', 'å£¬']:
            advice.append("é˜³å¹²æ—¥ä¸»ï¼Œé€‚åˆä¸»åŠ¨å‡ºå‡»ï¼Œç§¯æè¡ŒåŠ¨")
        else:
            advice.append("é˜´å¹²æ—¥ä¸»ï¼Œé€‚åˆç¨³å¥å‘å±•ï¼Œå¾ªåºæ¸è¿›")
        
        # æ ¹æ®å…­ç¥ç»™å‡ºå»ºè®®
        if shen in ['é’é¾™', 'è´µäºº']:
            advice.append("æœ‰è´µäººç›¸åŠ©ï¼Œå¯ä»¥å¤§èƒ†è¡ŒåŠ¨ï¼Œå¯»æ±‚å¸®åŠ©")
        elif shen in ['æœ±é›€', 'å¤ªå¸¸']:
            advice.append("æ–‡ä¹¦æœ‰åˆ©ï¼Œé€‚åˆç­¾çº¦ã€è€ƒè¯•ã€ç”³è¯·ç­‰")
        elif shen in ['å‹¾é™ˆ', 'è£è›‡']:
            advice.append("äº‹æƒ…å¤šå˜ï¼Œéœ€è¦çµæ´»åº”å¯¹ï¼Œé¿å…å†²çª")
        elif shen in ['ç™½è™', 'ç„æ­¦']:
            advice.append("æœ‰æŒ‘æˆ˜ï¼Œéœ€è¦åŠªåŠ›å…‹æœï¼Œä¿æŒè­¦æƒ•")
        
        # æ ¹æ®ä¸‰ä¼ ç»™å‡ºå»ºè®®
        if method == 'è´¼å…‹æ³•':
            advice.append("äº‹æƒ…æœ‰å˜åŒ–ï¼Œéœ€è¦çµæ´»åº”å¯¹")
        elif method == 'çŸ¥ä¸€æ³•':
            advice.append("æ–¹å‘æ˜ç¡®ï¼Œå¯ä»¥æœæ–­è¡ŒåŠ¨")
        elif method == 'æ¶‰å®³æ³•':
            advice.append("æœ‰å›°éš¾ï¼Œéœ€è¦è€å¿ƒå…‹æœ")
        elif method == 'åˆ«è´£æ³•':
            advice.append("æœ‰åˆ†æ­§ï¼Œéœ€è¦åè°ƒå¤„ç†")
        
        return "ï¼›".join(advice) if advice else "æ ¹æ®å…·ä½“æƒ…å†µçµæ´»åº”å¯¹"
    
    def _ai_intelligent_analysis(self, pan_result):
        """AIæ™ºèƒ½åˆ†æ"""
        return {
            'pattern_recognition': self._pattern_matching(pan_result),
            'prediction_analysis': self._ai_prediction(pan_result),
            'risk_assessment': self._ai_risk_assessment(pan_result),
            'success_probability': self._calculate_success_probability(pan_result),
            'smart_recommendations': self._generate_smart_recommendations(pan_result)
        }
    
    def _pattern_matching(self, pan_result):
        """æ¨¡å¼åŒ¹é…"""
        patterns = {
            'strong_leadership': ['ç”²', 'ä¸™', 'æˆŠ', 'é’é¾™', 'è´µäºº'],
            'stable_development': ['å·±', 'å¤ªå¸¸', 'å…­åˆ'],
            'creative_opportunity': ['ä¹™', 'ä¸', 'æœ±é›€'],
            'challenge_overcome': ['åºš', 'è¾›', 'ç™½è™'],
            'flexible_adaptation': ['å£¬', 'ç™¸', 'è£è›‡', 'å‹¾é™ˆ']
        }
        
        matched_patterns = []
        for pattern_name, pattern_elements in patterns.items():
            for element in pattern_elements:
                if (element in str(pan_result.get('ri_gan', '')) or 
                    element in str(pan_result.get('ri_zhi', '')) or
                    element in str(pan_result.get('liu_shen', {}).get('shen', ''))):
                    matched_patterns.append(pattern_name)
                    break
        
        return matched_patterns[:3]  # è¿”å›å‰3ä¸ªæœ€åŒ¹é…çš„æ¨¡å¼
    
    def _ai_prediction(self, pan_result):
        """AIé¢„æµ‹"""
        predictions = [
            "åŸºäºå†å²æ•°æ®åˆ†æï¼Œæ­¤æ’ç›˜æ˜¾ç¤ºäº‹æƒ…å‘å±•å°†è¾ƒä¸ºé¡ºåˆ©",
            "AIæ¨¡å‹é¢„æµ‹ï¼Œè¿‘æœŸå°†æœ‰é‡è¦æœºé‡å‡ºç°",
            "æ™ºèƒ½åˆ†ææ˜¾ç¤ºï¼Œéœ€è¦é‡ç‚¹å…³æ³¨äººé™…å…³ç³»å¤„ç†",
            "é¢„æµ‹ç»“æœæ˜¾ç¤ºï¼Œäº‹æƒ…å‘å±•éœ€è¦è€å¿ƒç­‰å¾…æ—¶æœº",
            "AIå»ºè®®ï¼Œé€‚åˆåœ¨è¿‘æœŸé‡‡å–ç§¯æè¡ŒåŠ¨"
        ]
        return random.choice(predictions)
    
    def _ai_risk_assessment(self, pan_result):
        """AIé£é™©è¯„ä¼°"""
        risk_levels = ['ä½é£é™©', 'ä¸­ä½é£é™©', 'ä¸­ç­‰é£é™©', 'ä¸­é«˜é£é™©', 'é«˜é£é™©']
        risk_factors = [
            "äººé™…å…³ç³»å¤æ‚",
            "å¤–éƒ¨ç¯å¢ƒå˜åŒ–",
            "å†…éƒ¨åè°ƒå›°éš¾",
            "èµ„æºä¸è¶³",
            "æ—¶æœºä¸æˆç†Ÿ"
        ]
        
        return {
            'risk_level': random.choice(risk_levels),
            'risk_factors': random.sample(risk_factors, random.randint(1, 3)),
            'mitigation_strategies': [
                "åŠ å¼ºæ²Ÿé€šåè°ƒ",
                "çµæ´»åº”å¯¹å˜åŒ–",
                "å¯»æ±‚å¤–éƒ¨æ”¯æŒ",
                "è€å¿ƒç­‰å¾…æ—¶æœº"
            ]
        }
    
    def _calculate_success_probability(self, pan_result):
        """è®¡ç®—æˆåŠŸæ¦‚ç‡"""
        base_probability = 0.6
        
        # æ ¹æ®å…­ç¥è°ƒæ•´æ¦‚ç‡
        liu_shen = pan_result.get('liu_shen', {})
        if isinstance(liu_shen, dict):
            shen = liu_shen.get('shen', '')
        else:
            shen = str(liu_shen)
        
        adjustments = {
            'é’é¾™': 0.2, 'è´µäºº': 0.2, 'å…­åˆ': 0.15,
            'æœ±é›€': 0.1, 'å¤ªå¸¸': 0.1,
            'å‹¾é™ˆ': -0.1, 'è£è›‡': -0.1,
            'ç™½è™': -0.15, 'ç„æ­¦': -0.15
        }
        
        adjustment = adjustments.get(shen, 0)
        final_probability = min(0.95, max(0.05, base_probability + adjustment))
        
        return round(final_probability, 2)
    
    def _generate_smart_recommendations(self, pan_result):
        """ç”Ÿæˆæ™ºèƒ½å»ºè®®"""
        recommendations = [
            "å»ºè®®åœ¨è¿‘æœŸé‡‡å–ç§¯æè¡ŒåŠ¨ï¼ŒæŠŠæ¡æœºé‡",
            "é‡ç‚¹å…³æ³¨äººé™…å…³ç³»å¤„ç†ï¼Œå¯»æ±‚åˆä½œæœºä¼š",
            "ä¿æŒè€å¿ƒï¼Œç­‰å¾…æœ€ä½³æ—¶æœºå†è¡ŒåŠ¨",
            "åŠ å¼ºæ²Ÿé€šåè°ƒï¼Œé¿å…ä¸å¿…è¦çš„å†²çª",
            "çµæ´»åº”å¯¹å˜åŒ–ï¼Œè°ƒæ•´ç­–ç•¥æ–¹å‘",
            "å¯»æ±‚å¤–éƒ¨æ”¯æŒï¼Œå€ŸåŠ©ä»–äººåŠ›é‡",
            "æ³¨é‡ç»†èŠ‚å¤„ç†ï¼Œé¿å…ç–å¿½å¤§æ„",
            "ä¿æŒç§¯æå¿ƒæ€ï¼Œç›¸ä¿¡è‡ªå·±çš„èƒ½åŠ›"
        ]
        
        return random.sample(recommendations, 3)
    
    def _case_based_analysis(self, pan_result):
        """åŸºäºæ¡ˆä¾‹çš„åˆ†æ"""
        similar_cases = self.find_similar_cases(pan_result, min_similarity=0.3, limit=5)
        
        if not similar_cases:
            return {
                'similar_cases': [],
                'historical_patterns': "æš‚æ— ç›¸ä¼¼æ¡ˆä¾‹",
                'success_insights': "å»ºè®®å‚è€ƒä¸€èˆ¬æ€§å»ºè®®",
                'case_recommendations': ["æ ¹æ®å…·ä½“æƒ…å†µçµæ´»åº”å¯¹"]
            }
        
        # åˆ†æå†å²æ¨¡å¼
        success_cases = [case for case in similar_cases if case['case_data'].get('accuracy', 0) >= 0.8]
        success_rate = len(success_cases) / len(similar_cases) if similar_cases else 0
        
        historical_patterns = f"åŸºäº{len(similar_cases)}ä¸ªç›¸ä¼¼æ¡ˆä¾‹åˆ†æï¼Œå†å²æˆåŠŸç‡çº¦ä¸º{success_rate:.1%}"
        
        # æå–æˆåŠŸæ´å¯Ÿ
        if success_cases:
            success_insights = "æˆåŠŸæ¡ˆä¾‹æ˜¾ç¤ºï¼šç§¯æè¡ŒåŠ¨ã€æŠŠæ¡æ—¶æœºã€å¯»æ±‚åˆä½œæ˜¯å…³é”®å› ç´ "
        else:
            success_insights = "å»ºè®®è°¨æ…è¡Œäº‹ï¼Œå……åˆ†å‡†å¤‡åå†è¡ŒåŠ¨"
        
        # ç”Ÿæˆæ¡ˆä¾‹å»ºè®®
        case_recommendations = [
            "å‚è€ƒç›¸ä¼¼æ¡ˆä¾‹çš„æˆåŠŸç»éªŒ",
            "é¿å…é‡å¤å†å²æ¡ˆä¾‹ä¸­çš„é”™è¯¯",
            "æ ¹æ®å½“å‰æƒ…å†µè°ƒæ•´ç­–ç•¥",
            "ä¿æŒçµæ´»æ€§å’Œé€‚åº”æ€§"
        ]
        
        return {
            'similar_cases': similar_cases,
            'historical_patterns': historical_patterns,
            'success_insights': success_insights,
            'case_recommendations': case_recommendations
        }
    
    def analyze(self, pan_result):
        """ä¸»åˆ†æå‡½æ•°"""
        cache_key = self._generate_cache_key(pan_result)
        
        if cache_key in self.analysis_cache:
            return self.analysis_cache[cache_key]
        
        # æ‰§è¡Œå„ç§åˆ†æ
        basic_analysis = self._basic_analysis(pan_result)
        ai_analysis = self._ai_intelligent_analysis(pan_result)
        case_analysis = self._case_based_analysis(pan_result)
        
        # ç»¼åˆç»“æœ
        comprehensive_result = {
            'basic_analysis': basic_analysis,
            'ai_analysis': ai_analysis,
            'case_analysis': case_analysis,
            'metadata': self._generate_metadata(pan_result),
            'confidence': self._calculate_overall_confidence(pan_result),
            'completeness': self._calculate_analysis_completeness()
        }
        
        # ç¼“å­˜ç»“æœ
        self.analysis_cache[cache_key] = comprehensive_result
        
        return comprehensive_result
    
    def _generate_metadata(self, pan_result):
        """ç”Ÿæˆå…ƒæ•°æ®"""
        return {
            'analysis_timestamp': datetime.now().isoformat(),
            'pan_result_summary': {
                'ri_gan': pan_result.get('ri_gan', ''),
                'ri_zhi': pan_result.get('ri_zhi', ''),
                'san_chuan_method': pan_result.get('san_chuan', {}).get('method_used', ''),
                'liu_shen': pan_result.get('liu_shen', {}).get('shen', '')
            },
            'analysis_version': '2.0',
            'database_size': len(self.cases) if hasattr(self, 'cases') else 0
        }
    
    def _calculate_overall_confidence(self, pan_result):
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # åŸºç¡€åˆ†æç½®ä¿¡åº¦
        confidence_factors.append(0.8)
        
        # AIåˆ†æç½®ä¿¡åº¦
        confidence_factors.append(0.7)
        
        # æ¡ˆä¾‹åŒ¹é…ç½®ä¿¡åº¦
        similar_cases = self.find_similar_cases(pan_result, min_similarity=0.3, limit=1)
        if similar_cases:
            confidence_factors.append(0.9)
        else:
            confidence_factors.append(0.5)
        
        return round(sum(confidence_factors) / len(confidence_factors), 2)
    
    def _calculate_analysis_completeness(self):
        """è®¡ç®—åˆ†æå®Œæ•´æ€§"""
        return 0.95  # 95%çš„å®Œæ•´æ€§ 